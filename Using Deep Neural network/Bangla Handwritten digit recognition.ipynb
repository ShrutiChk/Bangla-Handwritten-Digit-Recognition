{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "160204065_SC_Assignment2_Problem#1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la9EDK9lfpfX"
      },
      "source": [
        "# Problem : 01 : Bangla Handwritten digit recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZneFccpUf0-I"
      },
      "source": [
        "In problem 1 we used **\"NamtaDB**\" dataset to identify Bangla handwritten digits from 0 to 9. Here **Deep neural Network** is used to build a multiclassification model of supervised learning. \r\n",
        "\r\n",
        "**NumtaDB samples example**\r\n",
        "\r\n",
        "<div align=\"center\">\r\n",
        "<img src=\"https://drive.google.com/uc?id=1LvkNwV1My2RniR_JsbasBET1fa97eMQu\" width=\"500\">\r\n",
        "</div>\r\n",
        "\r\n",
        "- **Input dimension:**\r\n",
        "  - Size of image: $28 \\times 28 = 784$\r\n",
        "\r\n",
        "- **Output dimension: 10**\r\n",
        "  - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\r\n",
        "\r\n",
        "The NamtaDB dataset has total of **60k** data samples. Then the dataset is divided into **90:10** = train : test set using sckitlearn library. \r\n",
        "\r\n",
        "For this particular problem we have used *different hyperparameters* like - batch size, number of iterations, learning rate , optimizer etc to get the best results. The combination of hyperparameters and their acquired accuracy is given below - \r\n",
        "\r\n",
        "| Hyperparameters| Setting 1| Setting 2  | Setting 3 | Setting 4  |\r\n",
        "| :-------------:| :------: | :--------: | :-------: | :--------: | \r\n",
        "| Batch size     |    100   |    256      |    526     |    1024     |\r\n",
        "| No. of iterations|  3000  |    3000    |    5000   |    10000    |\r\n",
        "| Epoch          |    5.46  |    13.98    |    47.89   |    186.49   |\r\n",
        "| Learning Rate  |    0.1 |    0.1   |    0.0005  |    0.0005   |\r\n",
        "| Optimizer      |    SGD   |    Adam     |    Adam    |    Adamax     |\r\n",
        "| No of hidden Layers| 1  |    2     |    2    |    3     |\r\n",
        "| No of hidden neurons  |    100   |    256    |    128,64   |    256,128     |\r\n",
        "| Activation function|Tanh   |    Tanh,ReLU6     |    ReLU6,ReLU    |  Tanh,ReLU6,ReLU     |\r\n",
        "| ***Accuracy*** |   32.307 |    63.922  |    76.197 |    82.189  |\r\n",
        "\r\n",
        "                                                    Table : Results of different hyperparameters. \r\n",
        "\r\n",
        "\r\n",
        "In this problem different hyperparameter setup is used to see the **effect** of hyperparameters on the results of model which we can see in the above table. \r\n",
        "\r\n",
        "**Setting-1** has basic setup of hyperparameters. The batch size is **100** with **3000** iterations and **5.46** epochs. The learning rate is starting at **0.1** and **SGD** optimizer is used. For deep neural network here only one hidden layer is used with 100 neurons. It goes through Tanh actvation function.This basic model has an accuracy of **32.307** which is not quite satisfying. \r\n",
        "\r\n",
        "In **Setting-2** I have increased the batch size to **256** and the learning rate is same. After experimenting many setups it is seen that **Adam** optimizer works better than other optimizers for this problem. Therefore along with using Adam optimizer I have increased the no. of hidden layers to **2** and also the no. of hidden layer neurons to **256**. For these two layers two *different activation functions* - **Tanh and ReLU6** is used. Tanh works *better* than sigmoid and ReLU here and even ReLU6 works better than ReLU and SiLU. As a result a prominant increase in accuracy is seen which is **63.922%**. \r\n",
        "\r\n",
        "**Setting-3** has a **bigger** batch size to learn with big steps and the no of iterations are also increased to **5000** . After *several* setups it can be concluded that the model works best with **very small** learning rates. So here learning rate is set to **0.005** and the no. of hidden layer neurons is varied with **128** and **64** . For activation **ReLU6 and ReLU** is used. After **47.89** epochs the model has an accuracy of **76.197%**. \r\n",
        "\r\n",
        "After inreasing the batch size more the model performs better. So in this **Setting -4** the batch size is increased to **1024** and the number of iterations is also increased to **10000** with 186.49 epochs. For more epochs the model learns better. The **Adamax** optimizer is used here and the network is made more *deep* with **3** hidden layers.  The hidden neurons are varied here with **256->256 and 128** neurons on 1st,2nd and 3rd layer. Each layer has different activation functions. The previous activation functions which performed best i.e **Tanh,ReLU6,ReLU** is used here. With this setting the model gives an accuracy of **82.189%**. This is the best result among all the settings. But is iterations are more increased then the model gets overfitted. \r\n",
        "\r\n",
        "So we can come to a conclusion that with proper tuning of the hyperparameters this multiclassification model with Deep neural network can perform better. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFX4YVjzCoNo"
      },
      "source": [
        "##Implementation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKxVJQr3O6Jr"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import PIL\r\n",
        "#from google.colab import drive\r\n",
        "#drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "d8_ycP8RSqwX",
        "outputId": "3a8c523e-f4ee-4d74-a75f-56de97e44278"
      },
      "source": [
        "# Colab library to upload files to notebook\r\n",
        "from google.colab import files\r\n",
        "\r\n",
        "# Install Kaggle library\r\n",
        "!pip install -q kaggle\r\n",
        "\r\n",
        "\r\n",
        "# Upload kaggle API key file\r\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-57bb6b47-2aaf-49d5-91b7-d1da8240a0a0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-57bb6b47-2aaf-49d5-91b7-d1da8240a0a0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32W4blrbVlLs",
        "outputId": "bcaff6a1-f68f-4075-8686-c3b44a3fa42d"
      },
      "source": [
        "!mkdir -p ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle/\r\n",
        "!ls ~/.kaggle\r\n",
        "!chmod 600 /root/.kaggle/kaggle.json\r\n",
        "\r\n",
        "!kaggle datasets download -d BengaliAI/numta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "Downloading numta.zip to /content\n",
            " 99% 1.89G/1.91G [00:25<00:00, 63.7MB/s]\n",
            "100% 1.91G/1.91G [00:25<00:00, 81.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U4InCXmWe01"
      },
      "source": [
        "!mkdir numtaDB\r\n",
        "!unzip -q numta.zip -d numtaDB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "e2p8ttR5a5n2",
        "outputId": "49aa7579-dbd8-4c4e-c76f-1dee9d491ff2"
      },
      "source": [
        "a_csv = pd.read_csv('numtaDB/training-a.csv', usecols=['filename','digit'])\r\n",
        "b_csv = pd.read_csv('numtaDB/training-b.csv', usecols=['filename','digit'])\r\n",
        "c_csv = pd.read_csv('numtaDB/training-c.csv', usecols=['filename','digit'])\r\n",
        "d_csv = pd.read_csv('numtaDB/training-d.csv', usecols=['filename','digit'])\r\n",
        "e_csv = pd.read_csv('numtaDB/training-e.csv', usecols=['filename','digit'])\r\n",
        "a_csv.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>digit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a00000.png</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a00001.png</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a00002.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a00003.png</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a00004.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     filename  digit\n",
              "0  a00000.png      5\n",
              "1  a00001.png      3\n",
              "2  a00002.png      1\n",
              "3  a00003.png      7\n",
              "4  a00004.png      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPcLuPNlegfS",
        "outputId": "73563245-ee0a-4d35-cc98-3e1fd544d5aa"
      },
      "source": [
        "frames = [a_csv,c_csv,d_csv]\r\n",
        "dataset = pd.concat(frames,ignore_index=True)\r\n",
        "print(len(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "3dEEfXVeWPoy",
        "outputId": "bdd87943-e8a0-4128-dc15-5501ec18895d"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>digit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a00000.png</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a00001.png</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a00002.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a00003.png</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a00004.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     filename  digit\n",
              "0  a00000.png      5\n",
              "1  a00001.png      3\n",
              "2  a00002.png      1\n",
              "3  a00003.png      7\n",
              "4  a00004.png      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hgab7a4fuqx",
        "outputId": "34df782d-8d2d-455d-a385-e78c2ac630b2"
      },
      "source": [
        "!mkdir numtaDBDataset\r\n",
        "\r\n",
        "import shutil\r\n",
        "import os\r\n",
        "from os import path\r\n",
        "def completeDataset(folder_name):\r\n",
        "  src = 'numtaDB/' + folder_name + '/'\r\n",
        "  dir_folders = os.listdir(src)\r\n",
        "  for dir_name in dir_folders:\r\n",
        "    file_name = os.path.join(src, dir_name)\r\n",
        "    if os.path.isfile(file_name):\r\n",
        "      shutil.copy(file_name, 'numtaDBDataset')  \r\n",
        "      \r\n",
        "completeDataset('training-a')\r\n",
        "print('A Done')\r\n",
        "#completeDataset('training-b')\r\n",
        "#print('B Done')\r\n",
        "completeDataset('training-c')\r\n",
        "print('C Done')\r\n",
        "completeDataset('training-d')\r\n",
        "print('D Done')\r\n",
        "#completeDataset('training-e')\r\n",
        "#print('E Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A Done\n",
            "C Done\n",
            "D Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXe2U3VJ4yXx"
      },
      "source": [
        "train_transform = transforms.Compose([\r\n",
        "    transforms.Resize(28),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(0.5, 0.5)\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlFiZ0S5kawe"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, df, images_folder, transform = None):\r\n",
        "        self.df = df\r\n",
        "        self.images_folder = images_folder\r\n",
        "        self.transform = transform\r\n",
        "        \r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.df)\r\n",
        "    def __getitem__(self, index):\r\n",
        "        filename = self.df['filename'][index]\r\n",
        "        label = self.df['digit'][index]\r\n",
        "        image = PIL.Image.open(os.path.join(self.images_folder, filename)).convert('L')\r\n",
        "        if self.transform is not None:\r\n",
        "            image = self.transform(image)\r\n",
        "        return image, label\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nV_VOm4u87y",
        "outputId": "8c29637f-d042-4a41-d19b-71bff3b05e37"
      },
      "source": [
        "fullDataset = CustomDataset(dataset, 'numtaDBDataset',train_transform)\r\n",
        "\r\n",
        "print(len(fullDataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fixuslWyw3F4",
        "outputId": "f72b2441-3ca4-4af2-f5ba-404fdfeb11d4"
      },
      "source": [
        "fullDataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.6392,  0.6392,  0.6392,  0.6392,  0.6471,  0.6392,  0.6392,\n",
              "            0.6471,  0.6392,  0.6392,  0.6471,  0.6392,  0.6314,  0.6314,\n",
              "            0.6392,  0.6314,  0.6235,  0.6314,  0.6392,  0.6314,  0.6314,\n",
              "            0.6314,  0.6392,  0.6392,  0.6471,  0.6471,  0.6471,  0.6471],\n",
              "          [ 0.6471,  0.6471,  0.6471,  0.6471,  0.6471,  0.6471,  0.6471,\n",
              "            0.6471,  0.6471,  0.6471,  0.6392,  0.6392,  0.6392,  0.6392,\n",
              "            0.6314,  0.6314,  0.6314,  0.6314,  0.6392,  0.6392,  0.6471,\n",
              "            0.6471,  0.6392,  0.6392,  0.6392,  0.6392,  0.6392,  0.6471],\n",
              "          [ 0.6471,  0.6471,  0.6471,  0.6471,  0.6471,  0.6549,  0.6549,\n",
              "            0.6471,  0.6549,  0.6471,  0.6392,  0.6471,  0.6471,  0.6392,\n",
              "            0.6392,  0.6392,  0.6314,  0.6314,  0.6392,  0.6392,  0.6392,\n",
              "            0.6392,  0.6392,  0.6392,  0.6392,  0.6314,  0.6471,  0.6471],\n",
              "          [ 0.6471,  0.6549,  0.6549,  0.6549,  0.6549,  0.6549,  0.6549,\n",
              "            0.6549,  0.6549,  0.6471,  0.6549,  0.6549,  0.6627,  0.6471,\n",
              "            0.6549,  0.6471,  0.6471,  0.6549,  0.6549,  0.6549,  0.6471,\n",
              "            0.6392,  0.6471,  0.6471,  0.6471,  0.6471,  0.6471,  0.6471],\n",
              "          [ 0.6549,  0.6549,  0.6549,  0.6549,  0.6549,  0.6549,  0.6549,\n",
              "            0.6549,  0.6549,  0.6549,  0.6549,  0.6549,  0.6549,  0.6549,\n",
              "            0.6549,  0.6549,  0.6549,  0.6471,  0.6549,  0.6549,  0.6549,\n",
              "            0.6549,  0.6549,  0.6549,  0.6471,  0.6471,  0.6392,  0.6471],\n",
              "          [ 0.6549,  0.6627,  0.6627,  0.6549,  0.6549,  0.6549,  0.6549,\n",
              "            0.4902,  0.4902,  0.6549,  0.6627,  0.6627,  0.6627,  0.6549,\n",
              "            0.6549,  0.6549,  0.6549,  0.6549,  0.6549,  0.6549,  0.6549,\n",
              "            0.6471,  0.6471,  0.6471,  0.6471,  0.6471,  0.6471,  0.6471],\n",
              "          [ 0.6627,  0.6706,  0.6627,  0.6627,  0.6627,  0.6549,  0.5529,\n",
              "            0.0902,  0.2000,  0.6314,  0.6549,  0.6627,  0.6627,  0.6627,\n",
              "            0.6549,  0.6549,  0.6549,  0.6549,  0.6627,  0.6549,  0.6549,\n",
              "            0.6471,  0.6471,  0.6471,  0.6549,  0.6549,  0.6549,  0.6471],\n",
              "          [ 0.6627,  0.6627,  0.6627,  0.6549,  0.6549,  0.6235,  0.2627,\n",
              "            0.1765,  0.0431,  0.5294,  0.6471,  0.6471,  0.6627,  0.6392,\n",
              "            0.6157,  0.6549,  0.6549,  0.6549,  0.6627,  0.6549,  0.6627,\n",
              "            0.6627,  0.6549,  0.6549,  0.6471,  0.6471,  0.6392,  0.6549],\n",
              "          [ 0.6549,  0.6549,  0.6549,  0.6471,  0.6392,  0.4824,  0.1686,\n",
              "            0.4980,  0.1451,  0.2941,  0.6078,  0.6392,  0.6392,  0.3804,\n",
              "            0.2549,  0.6235,  0.6549,  0.6549,  0.6549,  0.6549,  0.6549,\n",
              "            0.6471,  0.6549,  0.6549,  0.6549,  0.6549,  0.6471,  0.6549],\n",
              "          [ 0.6471,  0.6549,  0.6549,  0.6471,  0.6078,  0.2392,  0.3647,\n",
              "            0.6157,  0.5059,  0.1529,  0.2314,  0.4039,  0.4667,  0.1137,\n",
              "            0.1137,  0.5216,  0.5922,  0.6314,  0.6471,  0.6471,  0.6471,\n",
              "            0.6471,  0.6549,  0.6549,  0.6471,  0.6549,  0.6549,  0.6549],\n",
              "          [ 0.6392,  0.6392,  0.6392,  0.6314,  0.4902,  0.0980,  0.5137,\n",
              "            0.6235,  0.6314,  0.5765,  0.3961,  0.2235,  0.1294, -0.1373,\n",
              "           -0.0667,  0.1686,  0.4039,  0.6235,  0.6314,  0.6314,  0.6392,\n",
              "            0.6392,  0.6392,  0.6471,  0.6471,  0.6392,  0.6471,  0.6471],\n",
              "          [ 0.6471,  0.6392,  0.6392,  0.6235,  0.2941,  0.2235,  0.6078,\n",
              "            0.6314,  0.6314,  0.6235,  0.6157,  0.6000,  0.4431, -0.0980,\n",
              "            0.2392,  0.5451,  0.6000,  0.6235,  0.6314,  0.6235,  0.6392,\n",
              "            0.6392,  0.6392,  0.6392,  0.6471,  0.6392,  0.6392,  0.6471],\n",
              "          [ 0.6392,  0.6392,  0.6314,  0.6078,  0.1529,  0.2784,  0.6157,\n",
              "            0.6235,  0.6235,  0.6235,  0.6235,  0.6157,  0.3961, -0.0510,\n",
              "            0.4275,  0.5608,  0.6157,  0.6235,  0.6314,  0.6314,  0.6392,\n",
              "            0.6392,  0.6392,  0.6392,  0.6471,  0.6471,  0.6471,  0.6471],\n",
              "          [ 0.6314,  0.6392,  0.6314,  0.5843,  0.0588,  0.3333,  0.6078,\n",
              "            0.6157,  0.6157,  0.6157,  0.6157,  0.6000,  0.3569,  0.0510,\n",
              "            0.5608,  0.6157,  0.6235,  0.6235,  0.6235,  0.6235,  0.6314,\n",
              "            0.6314,  0.6392,  0.6392,  0.6392,  0.6392,  0.6392,  0.6392],\n",
              "          [ 0.6392,  0.6314,  0.6235,  0.5765,  0.0353,  0.1922,  0.6000,\n",
              "            0.6078,  0.6000,  0.6000,  0.6078,  0.6078,  0.3176,  0.0667,\n",
              "            0.5608,  0.6000,  0.5922,  0.6000,  0.6078,  0.5843,  0.5922,\n",
              "            0.6235,  0.6314,  0.6314,  0.6314,  0.6314,  0.6392,  0.6314],\n",
              "          [ 0.6392,  0.6314,  0.6314,  0.6000,  0.1922,  0.0667,  0.5294,\n",
              "            0.6000,  0.6000,  0.5922,  0.5922,  0.5843,  0.3569,  0.0353,\n",
              "            0.5216,  0.5686,  0.5373,  0.4039,  0.2235,  0.0824,  0.3569,\n",
              "            0.6157,  0.6157,  0.6235,  0.6314,  0.6392,  0.6392,  0.6392],\n",
              "          [ 0.6392,  0.6392,  0.6314,  0.6235,  0.5059,  0.0510,  0.2000,\n",
              "            0.5373,  0.5843,  0.5922,  0.5843,  0.5843,  0.4824,  0.0588,\n",
              "            0.1451,  0.1451,  0.0196, -0.0902, -0.0667,  0.1216,  0.4980,\n",
              "            0.6078,  0.6157,  0.6157,  0.6314,  0.6314,  0.6314,  0.6314],\n",
              "          [ 0.6392,  0.6314,  0.6314,  0.6235,  0.6157,  0.4824,  0.1922,\n",
              "            0.1686,  0.2549,  0.3176,  0.3333,  0.3882,  0.3882,  0.2471,\n",
              "            0.0196, -0.0196,  0.0039,  0.1373,  0.3647,  0.5529,  0.6000,\n",
              "            0.6078,  0.6235,  0.6235,  0.6314,  0.6314,  0.6314,  0.6392],\n",
              "          [ 0.6392,  0.6314,  0.6314,  0.6235,  0.6235,  0.6078,  0.5843,\n",
              "            0.5137,  0.4510,  0.3647,  0.3098,  0.2392,  0.3020,  0.2941,\n",
              "            0.3412,  0.4353,  0.5059,  0.5608,  0.5843,  0.6000,  0.6078,\n",
              "            0.6157,  0.6235,  0.6314,  0.6314,  0.6314,  0.6314,  0.6392],\n",
              "          [ 0.6392,  0.6392,  0.6314,  0.6314,  0.6235,  0.6235,  0.6157,\n",
              "            0.6157,  0.6000,  0.6000,  0.5922,  0.5843,  0.5843,  0.5922,\n",
              "            0.5922,  0.5922,  0.5922,  0.6078,  0.6157,  0.6235,  0.6157,\n",
              "            0.6235,  0.6314,  0.6314,  0.6314,  0.6314,  0.6314,  0.6392],\n",
              "          [ 0.6392,  0.6314,  0.6314,  0.6314,  0.6314,  0.6235,  0.6235,\n",
              "            0.6157,  0.6078,  0.6157,  0.6157,  0.6157,  0.6157,  0.6078,\n",
              "            0.6078,  0.6078,  0.6078,  0.6157,  0.6157,  0.6235,  0.6235,\n",
              "            0.6235,  0.6314,  0.6314,  0.6314,  0.6392,  0.6392,  0.6392],\n",
              "          [ 0.6392,  0.6392,  0.6392,  0.6392,  0.6392,  0.6235,  0.6235,\n",
              "            0.6314,  0.6235,  0.6235,  0.6235,  0.6157,  0.6078,  0.6157,\n",
              "            0.6235,  0.6235,  0.6235,  0.6235,  0.6235,  0.6235,  0.6314,\n",
              "            0.6314,  0.6392,  0.6392,  0.6392,  0.6392,  0.6471,  0.6471],\n",
              "          [ 0.6471,  0.6392,  0.6392,  0.6392,  0.6392,  0.6392,  0.6235,\n",
              "            0.6314,  0.6314,  0.6314,  0.6314,  0.6235,  0.6235,  0.6235,\n",
              "            0.6235,  0.6235,  0.6235,  0.6235,  0.6314,  0.6314,  0.6314,\n",
              "            0.6471,  0.6471,  0.6392,  0.6392,  0.6471,  0.6471,  0.6392],\n",
              "          [ 0.6471,  0.6392,  0.6392,  0.6471,  0.6392,  0.6392,  0.6392,\n",
              "            0.6314,  0.6314,  0.6314,  0.6314,  0.6235,  0.6235,  0.6314,\n",
              "            0.6314,  0.6314,  0.6235,  0.6314,  0.6314,  0.6314,  0.6314,\n",
              "            0.6314,  0.6392,  0.6392,  0.6392,  0.6471,  0.6471,  0.6471],\n",
              "          [ 0.6471,  0.6392,  0.6471,  0.6392,  0.6392,  0.6392,  0.6392,\n",
              "            0.6392,  0.6392,  0.6314,  0.6314,  0.6314,  0.6314,  0.6314,\n",
              "            0.6314,  0.6314,  0.6235,  0.6314,  0.6314,  0.6314,  0.6314,\n",
              "            0.6314,  0.6392,  0.6392,  0.6392,  0.6471,  0.6392,  0.6471],\n",
              "          [ 0.6471,  0.6471,  0.6392,  0.6471,  0.6471,  0.6392,  0.6392,\n",
              "            0.6392,  0.6392,  0.6314,  0.6314,  0.6314,  0.6235,  0.6314,\n",
              "            0.6314,  0.6314,  0.6392,  0.6392,  0.6314,  0.6392,  0.6314,\n",
              "            0.6314,  0.6235,  0.6314,  0.6314,  0.6471,  0.6471,  0.6471],\n",
              "          [ 0.6392,  0.6471,  0.6471,  0.6471,  0.6392,  0.6392,  0.6392,\n",
              "            0.6392,  0.6392,  0.6314,  0.6392,  0.6392,  0.6314,  0.6314,\n",
              "            0.6314,  0.6314,  0.6392,  0.6392,  0.6392,  0.6392,  0.6314,\n",
              "            0.6314,  0.6314,  0.6392,  0.6471,  0.6549,  0.6471,  0.6471],\n",
              "          [ 0.6471,  0.6471,  0.6471,  0.6392,  0.6392,  0.6392,  0.6392,\n",
              "            0.6392,  0.6392,  0.6392,  0.6392,  0.6314,  0.6314,  0.6392,\n",
              "            0.6392,  0.6314,  0.6392,  0.6392,  0.6392,  0.6392,  0.6392,\n",
              "            0.6392,  0.6471,  0.6471,  0.6392,  0.6471,  0.6471,  0.6471]]]),\n",
              " 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qrLwk1srVZR"
      },
      "source": [
        "from sklearn.utils import shuffle\r\n",
        "fullDataset = shuffle(fullDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_RujX47XtIm",
        "outputId": "a9df622a-7551-411e-cd31-2ffac8bc97df"
      },
      "source": [
        "# split into train test sets\r\n",
        "train_dataset, test_dataset = train_test_split(fullDataset, test_size = 0.1)\r\n",
        "print(len(train_dataset))\r\n",
        "print(len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49417\n",
            "5491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMzS01Qz5vHu",
        "outputId": "091a35ea-0c79-4a33-d22c-884b59512497"
      },
      "source": [
        "print(train_dataset[51][0].size())\r\n",
        "print(train_dataset[51][0].numpy().shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "2weP4HEbnnrn",
        "outputId": "c1d11a1b-945f-439d-e5f0-34755541c237"
      },
      "source": [
        "#plotting image\r\n",
        "show_img = fullDataset[0][0].numpy().reshape(28, 28)\r\n",
        "plt.imshow(show_img, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3feee6d150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNaB_v4SiZ0R"
      },
      "source": [
        "### Setting-1  : \r\n",
        "\r\n",
        "- **totaldata:** 54908\r\n",
        "- **minibatch:** 100\r\n",
        "- **iterations:** 3,000\r\n",
        "- **epochs**\r\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{54908}{100} = 5.46 $\r\n",
        "- **Learning rate:** 0.1\r\n",
        "- **Optimizer:** SGD\r\n",
        "- **No. of hidden layer neurons :** 100\r\n",
        "-**No of hidden layers :** 1\r\n",
        "- **Activation function :** Tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Twkzw_PPQ9c"
      },
      "source": [
        "# Hyperparameters\r\n",
        "\r\n",
        "batch_size = 100\r\n",
        "num_iters = 3000\r\n",
        "input_dim = 28*28 # num_features = 784\r\n",
        "num_hidden = 100 # num of hidden nodes\r\n",
        "output_dim = 10\r\n",
        "\r\n",
        "learning_rate = 0.1  \r\n",
        "\r\n",
        "# Device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJXbgbZ9iFnr"
      },
      "source": [
        "'''\r\n",
        "MAKING DATASET ITERABLE\r\n",
        "'''\r\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\r\n",
        "num_epochs = int(num_epochs)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \r\n",
        "                                           batch_size=batch_size, \r\n",
        "                                           shuffle=True)  \r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \r\n",
        "                                          batch_size=batch_size, \r\n",
        "                                          shuffle=False)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ynx9tSUgxZF"
      },
      "source": [
        "### Designing the Model using class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKcLZtR5f8PW"
      },
      "source": [
        "class NeuralNetworkModel(nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\r\n",
        "        super().__init__()\r\n",
        "        ### 1st hidden layer\r\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\r\n",
        "\r\n",
        "        ### Non-linearity\r\n",
        "        self.sigmoid = nn.Sigmoid()\r\n",
        "\r\n",
        "        ### Output layer\r\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # Linear layer\r\n",
        "        out  = self.linear_1(x)\r\n",
        "        # Non-linearity\r\n",
        "        out = self.sigmoid(out)\r\n",
        "        # Linear layer (output)\r\n",
        "        probas  = self.linear_out(out)\r\n",
        "        return probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81NFKdjShCr-",
        "outputId": "8732aee5-b5bd-4dbe-8b5a-d82daed35c85"
      },
      "source": [
        "'''\r\n",
        "INSTANTIATE MODEL CLASS\r\n",
        "'''\r\n",
        "model = NeuralNetworkModel(input_size = input_dim,\r\n",
        "                           num_classes = output_dim,\r\n",
        "                           num_hidden = num_hidden)\r\n",
        "# To enable GPU\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetworkModel(\n",
              "  (linear_1): Linear(in_features=784, out_features=100, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (linear_out): Linear(in_features=100, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxmiLgYThjMK"
      },
      "source": [
        "###Constructing loss and optimizer (select from PyTorch API)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PHxefQ_hpbb"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS6aE0GfhvcI"
      },
      "source": [
        "### Training: forward, loss, backward, step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ameyx-ehs5P",
        "outputId": "67628f54-275d-49c9-d239-64b3131b0455"
      },
      "source": [
        "'''\r\n",
        "TRAIN THE MODEL\r\n",
        "'''\r\n",
        "iter = 0\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, labels) in enumerate(train_loader):\r\n",
        "\r\n",
        "        images = images.view(-1, 28*28).to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        # Clear gradients w.r.t. parameters\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # Forward pass to get output/logits\r\n",
        "        outputs = model(images.float()) \r\n",
        "\r\n",
        "        # Calculate Loss: softmax --> cross entropy loss\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "\r\n",
        "        # Getting gradients w.r.t. parameters\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Updating parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        iter += 1\r\n",
        "\r\n",
        "        if iter % 500 == 0:\r\n",
        "            # Calculate Accuracy         \r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            # Iterate through test dataset\r\n",
        "            for images, labels in test_loader:\r\n",
        "               \r\n",
        "                images = images.view(-1, 28*28).to(device)\r\n",
        "\r\n",
        "                # Forward pass only to get logits/output\r\n",
        "                outputs = model(images.float())\r\n",
        "\r\n",
        "                # Get predictions from the maximum value\r\n",
        "                _, predicted = torch.max(outputs, 1)\r\n",
        "\r\n",
        "                # Total number of labels\r\n",
        "                total += labels.size(0)\r\n",
        "\r\n",
        "\r\n",
        "                # Total correct predictions\r\n",
        "                if torch.cuda.is_available():\r\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \r\n",
        "                else:\r\n",
        "                    correct += (predicted == labels).sum()\r\n",
        "\r\n",
        "            accuracy = 100 * correct.item() / total\r\n",
        "\r\n",
        "            # Print Loss\r\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.2711846828460693. Accuracy: 17.282826443270807\n",
            "Iteration: 1000. Loss: 2.1721439361572266. Accuracy: 25.587324713167\n",
            "Iteration: 1500. Loss: 2.06144642829895. Accuracy: 23.638681478783464\n",
            "Iteration: 2000. Loss: 2.1383919715881348. Accuracy: 26.680021853942815\n",
            "Iteration: 2500. Loss: 1.9761675596237183. Accuracy: 32.30741212893826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GHp5Q-h5R5f"
      },
      "source": [
        "### Setting-2  : \r\n",
        "\r\n",
        "- **totaldata:** 54908\r\n",
        "- **minibatch:** 256\r\n",
        "- **iterations:** 3,000\r\n",
        "- **epochs**\r\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{54908}{256} = 13.98 $\r\n",
        "- **Learning rate:** 0.1\r\n",
        "- **Optimizer:** Adam\r\n",
        "- **No. of hidden layer neurons :** 256\r\n",
        "-**No of hidden layers :** 2\r\n",
        "- **Activation function :** Tanh -> ReLU6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxIUipuA7KdS",
        "outputId": "39921c77-85ee-4833-eba1-e05f1b644454"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.datasets as dsets\r\n",
        "\r\n",
        "# Hyperparameters\r\n",
        "batch_size = 256\r\n",
        "num_iters = 3000\r\n",
        "input_dim = 28*28 # num_features = 784\r\n",
        "num_hidden = 256\r\n",
        "output_dim = 10\r\n",
        "\r\n",
        "learning_rate = 0.001\r\n",
        "\r\n",
        "# Device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "from sklearn.utils import shuffle\r\n",
        "fullDataset = shuffle(fullDataset)\r\n",
        "train_dataset, test_dataset = train_test_split(fullDataset, test_size = 0.1)\r\n",
        "\r\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\r\n",
        "num_epochs = int(num_epochs)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \r\n",
        "                                           batch_size=batch_size, \r\n",
        "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \r\n",
        "                                          batch_size=batch_size, \r\n",
        "                                          shuffle=False) \r\n",
        "\r\n",
        "class DeepNeuralNetworkModel(nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\r\n",
        "        super().__init__()\r\n",
        "        ### 1st hidden layer: 784 --> 100\r\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\r\n",
        "        ### Non-linearity in 1st hidden layer\r\n",
        "        self.tanh_1 = nn.Tanh()\r\n",
        "\r\n",
        "        ### 2nd hidden layer: 100 --> 100\r\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\r\n",
        "        ### Non-linearity in 2nd hidden layer\r\n",
        "        self.relu_2 = nn.ReLU6()\r\n",
        "\r\n",
        "        ### Output layer: 100 --> 10\r\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        ### 1st hidden layer\r\n",
        "        out  = self.linear_1(x)\r\n",
        "        ### Non-linearity in 1st hidden layer\r\n",
        "        out = self.tanh_1(out)\r\n",
        "        \r\n",
        "        ### 2nd hidden layer\r\n",
        "        out  = self.linear_2(out)\r\n",
        "        ### Non-linearity in 2nd hidden layer\r\n",
        "        out = self.relu_2(out)\r\n",
        "        \r\n",
        "        # Linear layer (output)\r\n",
        "        probas  = self.linear_out(out)\r\n",
        "        return probas\r\n",
        "\r\n",
        "# INSTANTIATE MODEL CLASS\r\n",
        "\r\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\r\n",
        "                               num_classes = output_dim,\r\n",
        "                               num_hidden = num_hidden)\r\n",
        "# To enable GPU\r\n",
        "model.to(device)\r\n",
        "\r\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "iter = 0\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, labels) in enumerate(train_loader):\r\n",
        "\r\n",
        "        images = images.view(-1, 28*28).to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        # Clear gradients w.r.t. parameters\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # Forward pass to get output/logits\r\n",
        "        outputs = model(images) \r\n",
        "\r\n",
        "        # Calculate Loss: softmax --> cross entropy loss\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "\r\n",
        "        # Getting gradients w.r.t. parameters\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Updating parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        iter += 1\r\n",
        "\r\n",
        "        if iter % 500 == 0:\r\n",
        "            # Calculate Accuracy         \r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            # Iterate through test dataset\r\n",
        "            for images, labels in test_loader:\r\n",
        "               \r\n",
        "                images = images.view(-1, 28*28).to(device)\r\n",
        "\r\n",
        "                # Forward pass only to get logits/output\r\n",
        "                outputs = model(images)\r\n",
        "\r\n",
        "                # Get predictions from the maximum value\r\n",
        "                _, predicted = torch.max(outputs, 1)\r\n",
        "\r\n",
        "                # Total number of labels\r\n",
        "                total += labels.size(0)\r\n",
        "\r\n",
        "\r\n",
        "                # Total correct predictions\r\n",
        "                if torch.cuda.is_available():\r\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \r\n",
        "                else:\r\n",
        "                    correct += (predicted == labels).sum()\r\n",
        "\r\n",
        "            accuracy = 100 * correct.item() / total\r\n",
        "\r\n",
        "            # Print Loss\r\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 1.9613882303237915. Accuracy: 36.987798215261336\n",
            "Iteration: 1000. Loss: 1.6332391500473022. Accuracy: 47.149881624476414\n",
            "Iteration: 1500. Loss: 1.4556785821914673. Accuracy: 50.391549808778\n",
            "Iteration: 2000. Loss: 1.0802658796310425. Accuracy: 55.29047532325624\n",
            "Iteration: 2500. Loss: 1.1920934915542603. Accuracy: 63.92278273538518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w3xxQ_C6hcE"
      },
      "source": [
        "### Setting-3  : \r\n",
        "\r\n",
        "- **totaldata:** 54908\r\n",
        "- **minibatch:** 526\r\n",
        "- **iterations:** 5,000\r\n",
        "- **epochs**\r\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 5000 \\div \\frac{54908}{526} = 47.89 $\r\n",
        "- **Learning rate:** 0.0005\r\n",
        "- **Optimizer:** Adam\r\n",
        "- **No. of hidden layer neurons :** 128 -> 64\r\n",
        "-**No of hidden layers :** 2\r\n",
        "- **Activation function :** ReLU6 -> Relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7b2nqchkg-0",
        "outputId": "d04af76c-fb21-48be-cf3f-1a9d07b2ac32"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.datasets as dsets\r\n",
        "\r\n",
        "# Hyperparameters\r\n",
        "batch_size = 526\r\n",
        "num_iters = 5000\r\n",
        "input_dim = 28*28 # num_features = 784\r\n",
        "num_hidden = 128\r\n",
        "output_dim = 10\r\n",
        "\r\n",
        "learning_rate = 0.0005\r\n",
        "\r\n",
        "# Device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "from sklearn.utils import shuffle\r\n",
        "fullDataset = shuffle(fullDataset)\r\n",
        "train_dataset, test_dataset = train_test_split(fullDataset, test_size = 0.1)\r\n",
        "\r\n",
        "\r\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\r\n",
        "num_epochs = int(num_epochs)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \r\n",
        "                                           batch_size=batch_size, \r\n",
        "                                           shuffle=True)   \r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \r\n",
        "                                          batch_size=batch_size, \r\n",
        "                                          shuffle=False) \r\n",
        "\r\n",
        "class DeepNeuralNetworkModel(nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\r\n",
        "        super().__init__()\r\n",
        "        ### 1st hidden layer: 784 --> 128\r\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\r\n",
        "        ### Non-linearity in 1st hidden layer\r\n",
        "        self.relu_1 = nn.ReLU6()\r\n",
        "\r\n",
        "        ### 2nd hidden layer: 128 --> 64\r\n",
        "        self.linear_2 = nn.Linear(num_hidden, 64)\r\n",
        "        ### Non-linearity in 2nd hidden layer\r\n",
        "        self.relu_2 = nn.ReLU()\r\n",
        "\r\n",
        "        ### Output layer: 64 --> 10\r\n",
        "        self.linear_out = nn.Linear(64, num_classes)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        ### 1st hidden layer\r\n",
        "        out  = self.linear_1(x)\r\n",
        "        ### Non-linearity in 1st hidden layer\r\n",
        "        out = self.relu_1(out)\r\n",
        "        \r\n",
        "        ### 2nd hidden layer\r\n",
        "        out  = self.linear_2(out)\r\n",
        "        ### Non-linearity in 2nd hidden layer\r\n",
        "        out = self.relu_2(out)\r\n",
        "        \r\n",
        "        # Linear layer (output)\r\n",
        "        probas  = self.linear_out(out)\r\n",
        "        return probas\r\n",
        "model = NeuralNetworkModel(input_size = input_dim,\r\n",
        "                           num_classes = output_dim,\r\n",
        "                           num_hidden = num_hidden)\r\n",
        "# To enable GPU\r\n",
        "model.to(device)\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "iter = 0\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, labels) in enumerate(train_loader):\r\n",
        "\r\n",
        "        images = images.view(-1, 28*28).to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        # Clear gradients w.r.t. parameters\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # Forward pass to get output/logits\r\n",
        "        outputs = model(images.float()) \r\n",
        "\r\n",
        "        # Calculate Loss: softmax --> cross entropy loss\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "\r\n",
        "        # Getting gradients w.r.t. parameters\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Updating parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        iter += 1\r\n",
        "\r\n",
        "        if iter % 500 == 0:\r\n",
        "            # Calculate Accuracy         \r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            # Iterate through test dataset\r\n",
        "            for images, labels in test_loader:\r\n",
        "               \r\n",
        "                images = images.view(-1, 28*28).to(device)\r\n",
        "\r\n",
        "                # Forward pass only to get logits/output\r\n",
        "                outputs = model(images.float())\r\n",
        "\r\n",
        "                # Get predictions from the maximum value\r\n",
        "                _, predicted = torch.max(outputs, 1)\r\n",
        "\r\n",
        "                # Total number of labels\r\n",
        "                total += labels.size(0)\r\n",
        "\r\n",
        "\r\n",
        "                # Total correct predictions\r\n",
        "                if torch.cuda.is_available():\r\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \r\n",
        "                else:\r\n",
        "                    correct += (predicted == labels).sum()\r\n",
        "\r\n",
        "            accuracy = 100 * correct.item() / total\r\n",
        "\r\n",
        "            # Print Loss\r\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 1.7299050092697144. Accuracy: 43.87179020214897\n",
            "Iteration: 1000. Loss: 1.3950589895248413. Accuracy: 54.9080313239847\n",
            "Iteration: 1500. Loss: 1.1224157810211182. Accuracy: 62.11983245310508\n",
            "Iteration: 2000. Loss: 0.9529989957809448. Accuracy: 66.21744673101439\n",
            "Iteration: 2500. Loss: 0.8917745351791382. Accuracy: 69.36805682025133\n",
            "Iteration: 3000. Loss: 0.9308397769927979. Accuracy: 70.78856310325988\n",
            "Iteration: 3500. Loss: 0.7980701327323914. Accuracy: 70.69750500819522\n",
            "Iteration: 4000. Loss: 0.6663817167282104. Accuracy: 73.53851757421235\n",
            "Iteration: 4500. Loss: 0.6488248705863953. Accuracy: 76.19741395010017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j3VkfGz7nnu"
      },
      "source": [
        "### Setting-4  : \r\n",
        "\r\n",
        "- **totaldata:** 54908\r\n",
        "- **minibatch:** 1024\r\n",
        "- **iterations:** 10,000\r\n",
        "- **epochs**\r\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 10000 \\div \\frac{54908}{1024} = 186.49 $\r\n",
        "- **Learning rate:** 0.0005\r\n",
        "- **Optimizer:** Adamax\r\n",
        "- **No. of hidden layer neurons :** 256 -> 256 -> 128\r\n",
        "-**No of hidden layers :** 3\r\n",
        "- **Activation function :** Tanh, ReLU6, ReLU6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6njoVwtBCt8L",
        "outputId": "ce57cb09-ec4a-4576-afb7-2ecc1ec687fb"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.datasets as dsets\r\n",
        "\r\n",
        "# Hyperparameters\r\n",
        "batch_size = 1024\r\n",
        "num_iters = 10000\r\n",
        "input_dim = 28*28 # num_features = 784\r\n",
        "num_hidden = 256\r\n",
        "output_dim = 10\r\n",
        "\r\n",
        "learning_rate = 0.0005\r\n",
        "\r\n",
        "# Device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "from sklearn.utils import shuffle\r\n",
        "fullDataset = shuffle(fullDataset)\r\n",
        "train_dataset, test_dataset = train_test_split(fullDataset, test_size = 0.1)\r\n",
        "\r\n",
        "\r\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\r\n",
        "num_epochs = int(num_epochs)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \r\n",
        "                                           batch_size=batch_size, \r\n",
        "                                           shuffle=True)   \r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \r\n",
        "                                          batch_size=batch_size, \r\n",
        "                                          shuffle=False) \r\n",
        "\r\n",
        "class NeuralNetworkModel(nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\r\n",
        "        super().__init__()\r\n",
        "        ### 1st hidden layer\r\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\r\n",
        "        self.tanh_1= nn.Tanh()\r\n",
        "\r\n",
        "        ### 2nd hidden layer\r\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\r\n",
        "        self.relu_2= nn.ReLU6()\r\n",
        "\r\n",
        "        ### 3rd hidden layer\r\n",
        "        self.linear_3 = nn.Linear(num_hidden, 128)\r\n",
        "        self.relu_3= nn.ReLU6()\r\n",
        "\r\n",
        "        ### Output layer\r\n",
        "        self.linear_out = nn.Linear(128, num_classes)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # Linear layer\r\n",
        "        out  = self.linear_1(x)\r\n",
        "        # Non-linearity\r\n",
        "        out = self.tanh_1(out)\r\n",
        "\r\n",
        "        # Linear layer\r\n",
        "        out  = self.linear_2 (out)\r\n",
        "        # Non-linearity\r\n",
        "        out = self.relu_2(out)\r\n",
        "\r\n",
        "        out  = self.linear_3 (out)\r\n",
        "        # Non-linearity\r\n",
        "        out = self.relu_3(out)\r\n",
        "        # Linear layer (output)\r\n",
        "        probas  = self.linear_out(out)\r\n",
        "        return probas\r\n",
        "\r\n",
        "model = NeuralNetworkModel(input_size = input_dim,\r\n",
        "                           num_classes = output_dim,\r\n",
        "                           num_hidden = num_hidden)\r\n",
        "# To enable GPU\r\n",
        "model.to(device)\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "iter = 0\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, labels) in enumerate(train_loader):\r\n",
        "\r\n",
        "        images = images.view(-1, 28*28).to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        # Clear gradients w.r.t. parameters\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # Forward pass to get output/logits\r\n",
        "        outputs = model(images.float()) \r\n",
        "\r\n",
        "        # Calculate Loss: softmax --> cross entropy loss\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "\r\n",
        "        # Getting gradients w.r.t. parameters\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Updating parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        iter += 1\r\n",
        "\r\n",
        "        if iter % 500 == 0:\r\n",
        "            # Calculate Accuracy         \r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            # Iterate through test dataset\r\n",
        "            for images, labels in test_loader:\r\n",
        "               \r\n",
        "                images = images.view(-1, 28*28).to(device)\r\n",
        "\r\n",
        "                # Forward pass only to get logits/output\r\n",
        "                outputs = model(images.float())\r\n",
        "\r\n",
        "                # Get predictions from the maximum value\r\n",
        "                _, predicted = torch.max(outputs, 1)\r\n",
        "\r\n",
        "                # Total number of labels\r\n",
        "                total += labels.size(0)\r\n",
        "\r\n",
        "\r\n",
        "                # Total correct predictions\r\n",
        "                if torch.cuda.is_available():\r\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \r\n",
        "                else:\r\n",
        "                    correct += (predicted == labels).sum()\r\n",
        "\r\n",
        "            accuracy = 100 * correct.item() / total\r\n",
        "\r\n",
        "            # Print Loss\r\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 1.8648245334625244. Accuracy: 40.72118011291204\n",
            "Iteration: 1000. Loss: 1.5228160619735718. Accuracy: 50.173010380622834\n",
            "Iteration: 1500. Loss: 1.2551831007003784. Accuracy: 59.22418503004917\n",
            "Iteration: 2000. Loss: 1.070210576057434. Accuracy: 63.13968311782917\n",
            "Iteration: 2500. Loss: 0.9519976377487183. Accuracy: 67.05518120560917\n",
            "Iteration: 3000. Loss: 0.9001961946487427. Accuracy: 70.22400291385904\n",
            "Iteration: 3500. Loss: 0.7997441291809082. Accuracy: 72.88289928974686\n",
            "Iteration: 4000. Loss: 0.761428952217102. Accuracy: 74.88617738116919\n",
            "Iteration: 4500. Loss: 0.6848440766334534. Accuracy: 76.3431069022036\n",
            "Iteration: 5000. Loss: 0.661502480506897. Accuracy: 77.16262975778547\n",
            "Iteration: 5500. Loss: 0.6254251003265381. Accuracy: 77.52686213804407\n",
            "Iteration: 6000. Loss: 0.5583562850952148. Accuracy: 79.58477508650519\n",
            "Iteration: 6500. Loss: 0.5949784517288208. Accuracy: 79.96721908577672\n",
            "Iteration: 7000. Loss: 0.540980339050293. Accuracy: 80.85958841741031\n",
            "Iteration: 7500. Loss: 0.5143009424209595. Accuracy: 80.87780003642324\n",
            "Iteration: 8000. Loss: 0.5006112456321716. Accuracy: 81.77016936805683\n",
            "Iteration: 8500. Loss: 0.45990729331970215. Accuracy: 82.00692041522491\n",
            "Iteration: 9000. Loss: 0.4685949385166168. Accuracy: 81.2238207976689\n",
            "Iteration: 9500. Loss: 0.40388023853302. Accuracy: 83.55490803132399\n",
            "Iteration: 10000. Loss: 0.43067100644111633. Accuracy: 82.18903660535422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnN0-2S7F7BA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}